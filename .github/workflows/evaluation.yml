name: Model Evaluation

on:
  pull_request:
    branches: [main, develop]
    paths:
      - 'src/agents/**'
      - 'src/evaluation/**'
      - 'prompts/**'
      - 'tests/test_evaluation/**'
  workflow_dispatch:
    inputs:
      run_full_suite:
        description: 'Run full evaluation suite'
        required: false
        default: 'false'
        type: boolean
      evaluation_tags:
        description: 'Evaluation tags to filter (comma-separated)'
        required: false
        type: string

env:
  PYTHON_VERSION: '3.12'
  EVALUATION_THRESHOLD: '0.65'
  REGRESSION_THRESHOLD: '0.05'

jobs:
  evaluate:
    name: Run Model Evaluation
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Install dependencies
        run: |
          uv sync
          uv pip install -e .

      - name: Run evaluation suite
        id: evaluation
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          EVALUATION_RUN_ID: ${{ github.run_id }}-${{ github.run_attempt }}
        run: |
          # Determine evaluation scope
          if [[ "${{ github.event.inputs.run_full_suite }}" == "true" ]]; then
            EVAL_ARGS="--full"
          else
            EVAL_ARGS="--quick"
          fi

          # Add tag filter if specified
          if [[ -n "${{ github.event.inputs.evaluation_tags }}" ]]; then
            EVAL_ARGS="$EVAL_ARGS --tags ${{ github.event.inputs.evaluation_tags }}"
          fi

          # Run evaluation and capture output
          uv run python -m src.evaluation.cli run \
            --run-id "$EVALUATION_RUN_ID" \
            --threshold ${{ env.EVALUATION_THRESHOLD }} \
            --output evaluation_results.json \
            $EVAL_ARGS || EXIT_CODE=$?

          # Parse results for GitHub output
          if [ -f evaluation_results.json ]; then
            PASS_RATE=$(jq -r '.pass_rate' evaluation_results.json)
            TOTAL_CASES=$(jq -r '.total_cases' evaluation_results.json)
            PASSED_CASES=$(jq -r '.passed_cases' evaluation_results.json)
            FAILED_CASES=$(jq -r '.failed_cases' evaluation_results.json)
            AGGREGATE_SCORE=$(jq -r '.mean_scores.aggregate // 0' evaluation_results.json)

            echo "pass_rate=$PASS_RATE" >> $GITHUB_OUTPUT
            echo "total_cases=$TOTAL_CASES" >> $GITHUB_OUTPUT
            echo "passed_cases=$PASSED_CASES" >> $GITHUB_OUTPUT
            echo "failed_cases=$FAILED_CASES" >> $GITHUB_OUTPUT
            echo "aggregate_score=$AGGREGATE_SCORE" >> $GITHUB_OUTPUT
          fi

          exit ${EXIT_CODE:-0}

      - name: Check for regression
        id: regression
        if: github.event_name == 'pull_request'
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          # Compare against baseline from main branch
          uv run python -m src.evaluation.cli regression \
            --current evaluation_results.json \
            --threshold ${{ env.REGRESSION_THRESHOLD }} \
            --output regression_report.json || true

          if [ -f regression_report.json ]; then
            HAS_REGRESSION=$(jq -r '.has_regression' regression_report.json)
            REGRESSION_COUNT=$(jq -r '.regressions | length' regression_report.json)
            IMPROVEMENT_COUNT=$(jq -r '.improvements | length' regression_report.json)

            echo "has_regression=$HAS_REGRESSION" >> $GITHUB_OUTPUT
            echo "regression_count=$REGRESSION_COUNT" >> $GITHUB_OUTPUT
            echo "improvement_count=$IMPROVEMENT_COUNT" >> $GITHUB_OUTPUT
          fi

      - name: Generate evaluation report
        id: report
        run: |
          # Generate markdown report
          cat << EOF > evaluation_report.md
          ## Model Evaluation Results

          | Metric | Value |
          |--------|-------|
          | Total Cases | ${{ steps.evaluation.outputs.total_cases }} |
          | Passed | ${{ steps.evaluation.outputs.passed_cases }} |
          | Failed | ${{ steps.evaluation.outputs.failed_cases }} |
          | Pass Rate | ${{ steps.evaluation.outputs.pass_rate }} |
          | Aggregate Score | ${{ steps.evaluation.outputs.aggregate_score }} |

          ### Quality Threshold
          - Required: ${{ env.EVALUATION_THRESHOLD }}
          - Actual: ${{ steps.evaluation.outputs.aggregate_score }}
          EOF

          if [[ "${{ steps.regression.outputs.has_regression }}" == "true" ]]; then
            cat << EOF >> evaluation_report.md

          ### ⚠️ Regression Detected
          - Regressions: ${{ steps.regression.outputs.regression_count }}
          - Improvements: ${{ steps.regression.outputs.improvement_count }}

          See detailed regression report in artifacts.
          EOF
          fi

          # Add dimension breakdown if available
          if [ -f evaluation_results.json ]; then
            echo "" >> evaluation_report.md
            echo "### Score Breakdown by Dimension" >> evaluation_report.md
            echo "" >> evaluation_report.md
            echo "| Dimension | Mean | Std |" >> evaluation_report.md
            echo "|-----------|------|-----|" >> evaluation_report.md
            jq -r '.mean_scores | to_entries[] | "| \(.key) | \(.value | . * 100 | floor / 100) | " + (input.std_scores[.key] // 0 | . * 100 | floor / 100 | tostring) + " |"' evaluation_results.json evaluation_results.json >> evaluation_report.md 2>/dev/null || true
          fi

          cat evaluation_report.md

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('evaluation_report.md', 'utf8');

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('## Model Evaluation Results')
            );

            const body = report + '\n\n---\n*Run ID: ${{ github.run_id }}*';

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Upload evaluation artifacts
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results-${{ github.run_id }}
          path: |
            evaluation_results.json
            regression_report.json
            evaluation_report.md
          retention-days: 30

      - name: Check quality gate
        if: github.event_name == 'pull_request'
        run: |
          PASS_RATE="${{ steps.evaluation.outputs.pass_rate }}"
          THRESHOLD="${{ env.EVALUATION_THRESHOLD }}"
          HAS_REGRESSION="${{ steps.regression.outputs.has_regression }}"

          # Check pass rate meets threshold
          if (( $(echo "$PASS_RATE < $THRESHOLD" | bc -l) )); then
            echo "::error::Evaluation pass rate ($PASS_RATE) is below threshold ($THRESHOLD)"
            exit 1
          fi

          # Check for regressions (warning only, not blocking)
          if [[ "$HAS_REGRESSION" == "true" ]]; then
            echo "::warning::Quality regression detected. Review the evaluation report."
          fi

          echo "✅ Quality gate passed!"

  store-results:
    name: Store Evaluation Results
    runs-on: ubuntu-latest
    needs: evaluate
    if: always() && needs.evaluate.result == 'success'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download evaluation artifacts
        uses: actions/download-artifact@v4
        with:
          name: evaluation-results-${{ github.run_id }}

      - name: Store results in database
        env:
          DATABASE_URL: ${{ secrets.EVALUATION_DATABASE_URL }}
        run: |
          # Store results for historical analysis
          # This would typically call an API or directly insert into a database
          echo "Storing evaluation results for run ${{ github.run_id }}"

          # For now, just validate the results file exists
          if [ -f evaluation_results.json ]; then
            echo "Results file found, size: $(wc -c < evaluation_results.json) bytes"
          fi
